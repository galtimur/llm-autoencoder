ModelArguments:
  model_name_or_path: "microsoft/phi-1_5"
  lora: false
  lora_r: 128
  lora_alpha: 32
  lora_dropout: 0.05
  lora_bias: "none"
#  train: true

DataArguments:
  data_path: null
  debug_data: false
  train_dataset_name: "openwebtext"
  train_dataset_subset: null
  val_dataset_name: "wikitext"
  val_dataset_subset: "wikitext-103-raw-v1"
  text_key: "text"
  rnd_seed: 42

TrainingArguments:
  cache_dir: null
  output_dir: /mnt/data2/galimzyanov/llm-autoencoder/output
  optim: "adamw_torch"
  batch_size_mini: 10
  batch_size_global: 100
  batch_size_outer: 1
  num_train_epochs: 1
  segment_length: 128
  compression_rate: 4
  restore_from: ""
  bf16: true
  wandb_project_name: "autoencoder"
  eval_steps: 2000
  max_eval_steps: 600
  learning_rate: 1e-3