ModelArguments:
  model_name_or_path: "microsoft/phi-1_5"
  pretrained_encoder: false
  pretrained_decoder: false
  freeze_encoder: false
  freeze_decoder: false
  share_enc_dec: false
  init_same_weights: false
  use_linear_layer: true
  alter_model: true
  hidden_size: 768
  num_layers: 16
  lora_encoder: false
  lora_decoder: false
  lora_r: 16
  lora_alpha: 16
  lora_dropout: 0.05
  lora_target_modules: ["q_proj", "v_proj", "o_proj", "k_proj"]
  lora_bias: "none"
#  train: true

DataArguments:
  data_path: null
  debug_data: false
  train_dataset_name: "openwebtext"
  train_dataset_subset: null
  val_dataset_name: "wikitext"
  val_dataset_subset: "wikitext-103-raw-v1"
  text_key: "text"
  rnd_seed: 42

TrainingArguments:
  cache_dir: null
  output_dir: /mnt/data2/galimzyanov/llm-autoencoder/output
  optim: "adamw_torch"
  batch_size_mini: 50
  batch_size_global: 500
  batch_size_outer: 5
  num_train_epochs: 1
  segment_length: 128
  compression_rate: 4
  restore_from: ""
  bf16: true
  wandb_project_name: "autoencoder"
  save_steps: 100
  eval_steps: 2000
  max_eval_steps: 600
  learning_rate: 1e-3