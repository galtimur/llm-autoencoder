ModelArguments:
  encoder_name_or_path: "deepseek-ai/deepseek-coder-1.3b-base"
  decoder_name_or_path: "deepseek-ai/deepseek-coder-6.7b-base"
  # model_weights_path: /mnt/data2/galimzyanov/llm-autoencoder/output/auenc-4x # null
  task_type: "autocompressor" # options: "autoencoder", "autocompressor", "base_no_context"
  pretrained_encoder: true
  pretrained_decoder: true
  freeze_encoder: false
  freeze_decoder: true
  freeze_linear: false
  freeze_summary: false
  share_enc_dec: false
  init_same_weights: false
  use_linear_layer: true
  lora_encoder: true
  lora_decoder: false
  lora_r: 16
  lora_alpha: 16
  lora_dropout: 0.05
  lora_target_modules: ["q_proj", "v_proj", "o_proj", "k_proj"]
  lora_bias: "none"
#  train: true

DataArguments:
  data_path: null
  debug_data: false
  train_dataset_name: "bigcode/the-stack-dedup"# "openwebtext", "bigcode/the-stack-dedup"
  train_dataset_subname: null # null,
  train_dataset_subdir: "data/python"
  validate_ce: true
  validate_em: false
  val_dataset_name: "train"# "wikitext"
  # val_dataset_subname: "wikitext-103-raw-v1"
  text_key: "content"
  rnd_seed: 42

TrainingArguments:
  cache_dir: null
  output_dir: /mnt/data2/galimzyanov/llm-autoencoder/output/auco-4x-hybrid_DS-1.3-6.7
  optim: "adamw_torch"
  batch_size_mini: 5
  batch_size_global: 100
  batch_size_outer: 1
  num_train_epochs: 1
  segment_length: 128
  compression_rate: 10
  bf16: true
  wandb_project_name: "autoencoder"
  save_steps: 10000
  eval_steps: 3000
  max_eval_steps: 1000
  max_eval_samples: 100000
  learning_rate: 1e-3